{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FacialLandmarks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Waidhoferj/CSC-566-Project/blob/facial-filtering/FacialLandmarks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EJZwDvM21EV"
      },
      "source": [
        "# Facial Landmarks\n",
        "Experimentation with facial landmarks models and datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQFjoPKY-ZaY"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Circle\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras import layers\n",
        "from google.colab import drive\n",
        "import os\n",
        "from scipy.io import loadmat\n",
        "import random"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCIThEi--w0H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a67b600-c9a0-4a74-be1a-7925bcd570aa"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXe9WUnF2_O3"
      },
      "source": [
        "Add your name and filepath to the project folder so that you can load the datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngxIBPhm1Q80"
      },
      "source": [
        "USER = \"Ty\"\n",
        "USER_FILEPATHS = {\n",
        "    \"John\" : \"/content/drive/MyDrive/CSC 566 Project\",\n",
        "    \"Jeremy\" : \"/content/drive/MyDrive/School/Undergrad/2021 Spring/CSC 566/CSC 566 Project\",\n",
        "    \"Ty\" : \"/content/drive/MyDrive/CSC 566 Project\"\n",
        "}\n",
        "PROJECT_FILEPATH = USER_FILEPATHS[USER]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcX_vDf3BcjR"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpSQj0yh2UTF"
      },
      "source": [
        "# Sanity check for loading data\n",
        "def display_landmarks(img, points):\n",
        "  \"\"\"\n",
        "  Displays portrait with landmark dots drawn on the face.\n",
        "  Assumes that points are in (68,2)\n",
        "  \"\"\"\n",
        "  #If we are reading from .mat files directly (2,68)\n",
        "  if points.shape[0] == 2:\n",
        "    points = points.transpose(1,0)\n",
        "  #If reading from model output\n",
        "  elif len(points.shape) == 1:\n",
        "    points = points.reshape(-1,2)\n",
        "  fig,ax = plt.subplots(1)\n",
        "  ax.set_aspect('equal')\n",
        "  ax.imshow(img)\n",
        "  for p in points:\n",
        "      circ = Circle(p)\n",
        "      ax.add_patch(circ)\n",
        "  plt.show()\n",
        "\n",
        "#points = loadmat(AFW_DATASET + \"/70037463_1.mat\")[\"pt2d\"]\n",
        "#img  = plt.imread( AFW_DATASET+\"/70037463_1.jpg\")\n",
        "\n",
        "#display_landmarks(img,points)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-39rOKd3deP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b2dca4d-02af-4567-c222-27db6516d3c3"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "DATALOADER_BASE_PATH = PROJECT_FILEPATH + \"/datasets/tf-datasets\"\n",
        "USE_CROPPED_IMAGES = True\n",
        "DATASET_PREFIX = \"cropped-\" if USE_CROPPED_IMAGES else \"\"\n",
        "\n",
        "class DataLoader:\n",
        "\n",
        "  IMAGE_SHAPE = (256, 256, 3) if USE_CROPPED_IMAGES else (450, 450, 3)\n",
        "  LANDMARKS_SHAPE = (136,)\n",
        "\n",
        "  BATCH_SIZE = 64\n",
        "  TRAIN_PATHS = [f\"{DATALOADER_BASE_PATH}/{DATASET_PREFIX}train-{i}.tfrecord.gz\" for i in range(5)]\n",
        "  VAL_PATHS = [f\"{DATALOADER_BASE_PATH}/{DATASET_PREFIX}val-{i}.tfrecord.gz\" for i in range(5)]\n",
        "  TEST_PATHS = [f\"{DATALOADER_BASE_PATH}/{DATASET_PREFIX}test-{i}.tfrecord.gz\" for i in range(5)]\n",
        "\n",
        "  def load_datasets():\n",
        "    dl = DataLoader()\n",
        "    return (dl.__load_dataset(DataLoader.TRAIN_PATHS, DataLoader.BATCH_SIZE),\n",
        "            dl.__load_dataset(DataLoader.VAL_PATHS, DataLoader.BATCH_SIZE),\n",
        "            dl.__load_dataset(DataLoader.TEST_PATHS, 1))\n",
        "\n",
        "  def __load_dataset(self, filepath, batch_size):\n",
        "    dataset = tf.data.TFRecordDataset([filepath], compression_type=\"GZIP\")\n",
        "    dataset = dataset.map(self.__parse_example, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.map(self.__reshape_entry, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    images = dataset.map(lambda x,y: x).batch(DataLoader.BATCH_SIZE)\n",
        "    labels = dataset.map(lambda x,y: y).batch(DataLoader.BATCH_SIZE)\n",
        "    dataset = tf.data.Dataset.zip((images, labels))\n",
        "    dataset.cache() # Cache the above map operations so they aren't re-run every epoch\n",
        "    return dataset\n",
        "\n",
        "\n",
        "  def __parse_example(self, record):\n",
        "    feature_names = {}\n",
        "    feature_names['image'] = tf.io.FixedLenSequenceFeature([],tf.float32, allow_missing=True)\n",
        "    feature_names['landmarks'] = tf.io.FixedLenSequenceFeature([],tf.float32, allow_missing=True)\n",
        "    return tf.io.parse_single_example(record, feature_names)\n",
        "\n",
        "  def __reshape_entry(self, entry):\n",
        "    image = tf.reshape(entry['image'], DataLoader.IMAGE_SHAPE)\n",
        "    landmarks = tf.reshape(entry['landmarks'], DataLoader.LANDMARKS_SHAPE)\n",
        "    return image, landmarks\n",
        "\n",
        "\n",
        "train_data, val_data, test_data = DataLoader.load_datasets()\n",
        "for i,record in enumerate(val_data):\n",
        "  print(record[0].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 450, 450, 3)\n",
            "(64, 450, 450, 3)\n",
            "(64, 450, 450, 3)\n",
            "(58, 450, 450, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwW6y33R9jmf"
      },
      "source": [
        "## Basic Benchmark Model\n",
        "From [this medium article](https://towardsdatascience.com/detecting-facial-features-using-deep-learning-2e23c8660a7a)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rX_z1wLO2YfP"
      },
      "source": [
        "def create_basic_landmark_model(input_shape, conv_range):\n",
        "  input_layer = layers.Input(input_shape)\n",
        "  x = input_layer\n",
        "  for exp in conv_range:\n",
        "    x = layers.Conv2D(2**exp, (3,3), 3, activation=\"relu\")(x)\n",
        "    x = layers.MaxPool2D(padding=\"same\")(x)\n",
        "  x = layers.Flatten()(x)\n",
        "  x = layers.Dense(500, activation=\"relu\")(x)\n",
        "  x = layers.Dense(90, activation=\"relu\")(x)\n",
        "  x = layers.Dense(68*2, activation=\"relu\")(x)\n",
        "  return Model(name=\"landmark_locator\", inputs=input_layer, outputs=x)\n",
        "\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfkeiXIDEfgL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee665e6f-6800-47bf-bb58-0c2a2c30bdf9"
      },
      "source": [
        "INPUT_SHAPE = (256, 256, 3) \n",
        "basic_model = create_basic_landmark_model(INPUT_SHAPE, range(5,8))\n",
        "basic_model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"accuracy\"])\n",
        "basic_model.summary()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"landmark_locator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 256, 256, 3)]     0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 85, 85, 32)        896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 43, 43, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 14, 14, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 2, 2, 128)         73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 1, 1, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 500)               64500     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 90)                45090     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 136)               12376     \n",
            "=================================================================\n",
            "Total params: 215,214\n",
            "Trainable params: 215,214\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQc5qiW1FzcD"
      },
      "source": [
        "# Training\n",
        "basic_model.fit(train_data.shuffle(64), epochs=30, validation_data=val_data, verbose=1)\n",
        "basic_model.save_weights(PROJECT_FILEPATH + \"/basic-weights.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmyv0MlwCFIN"
      },
      "source": [
        "# Running\n",
        "basic_model.load_weights(PROJECT_FILEPATH + \"/basic-weights.h5\")"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KH-M1YNKhv-l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65e6e601-c346-4975-e607-a795fe46f293"
      },
      "source": [
        "import cv2 as cv\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# load the pre-trained model from https://machinelearningmastery.com/how-to-perform-face-detection-with-classical-and-deep-learning-methods-in-python-with-keras/\n",
        "classifier = cv.CascadeClassifier(cv.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "cap = cv.VideoCapture(PROJECT_FILEPATH + \"/webCamData/inputVideo.mp4\")\n",
        "if not cap.isOpened():\n",
        "    print(\"Cannot open camera\")\n",
        "    exit()\n",
        "\n",
        "frameCount = 0\n",
        "test_data = []\n",
        "while True:\n",
        "    # Capture frame-by-frame\n",
        "    ret, frame = cap.read()\n",
        "    # if frame is read correctly ret is True\n",
        "    if not ret:\n",
        "        print(\"Can't receive frame (stream end?). Exiting ...\")\n",
        "        break\n",
        "\n",
        "    #grayscale image\n",
        "    gray_image = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
        "\n",
        "    #face_cascade.detectMultiScale requires unisigned integer\n",
        "    gray_image = np.array(gray_image, dtype='uint8') \n",
        "\n",
        "    #Testing purposes DELETE\n",
        "    #from matplotlib import pyplot as plt\n",
        "    #plt.imshow(gray_image)\n",
        "    #plt.show()\n",
        "\n",
        "    # perform face detection\n",
        "    bboxes = classifier.detectMultiScale(gray_image)\n",
        "\n",
        "    # using the bounding box, crop the image for each detected face\n",
        "    for box in bboxes:\n",
        "      x, y, width, height = box\n",
        "      x2, y2 = x + width, y + height\n",
        "      cropped_img = gray_image[y:y+height, x:x+width]\n",
        "\n",
        "    test_data.append(cropped_img)\n",
        "\n",
        "    #Testing purposes: DELETE\n",
        "    #from matplotlib import pyplot as plt\n",
        "    #plt.imshow(cropped_img)\n",
        "    #plt.show()\n",
        "\n",
        "    # Saving the image\n",
        "    cv.imwrite(PROJECT_FILEPATH + \"/webCamData/webCamFrames/frame_\" + str(frameCount) + \".jpg\", cropped_img)\n",
        "    frameCount = frameCount + 1\n",
        "    \n",
        "# When everything done, release the capture\n",
        "cap.release()\n",
        "cv.destroyAllWindows()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Can't receive frame (stream end?). Exiting ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Bht0I-HuSle",
        "outputId": "e93140a7-58b5-4f6a-fa08-6ac0b1692b4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        }
      },
      "source": [
        "tf.convert_to_tensor(np.array(test_data))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-c5bee7e0aa5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1429\u001b[0m   \"\"\"\n\u001b[1;32m   1430\u001b[0m   return convert_to_tensor_v2(\n\u001b[0;32m-> 1431\u001b[0;31m       value, dtype=dtype, dtype_hint=dtype_hint, name=name)\n\u001b[0m\u001b[1;32m   1432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1439\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1441\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1566\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    263\u001b[0m   \"\"\"\n\u001b[1;32m    264\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 265\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODwkTNrjqkKV"
      },
      "source": [
        "predictions = basic_model.predict(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nr31eAIQVGlu"
      },
      "source": [
        "### Analyze Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukAD1nioU9WQ"
      },
      "source": [
        "pd.DataFrame(basic_model.history.history).plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQJxDEObVSOx"
      },
      "source": [
        "predictions = basic_model.predict(test_data.map(lambda x,y: x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AShlJ00Vz81"
      },
      "source": [
        "#@title View Predicted Images\n",
        "image_index = 3 #@param {type:\"slider\", min:1, max:16, step:1}\n",
        "points = predictions * 450\n",
        "i = 0\n",
        "for x in test_data.map(lambda x,y: x):\n",
        "  if (i == image_index):\n",
        "    display_landmarks(tf.reshape(x, DataLoader.IMAGE_SHAPE), points[image_index])\n",
        "    break\n",
        "  i += 1"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}