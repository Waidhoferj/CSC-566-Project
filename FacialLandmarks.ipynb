{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FacialLandmarks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Waidhoferj/CSC-566-Project/blob/main/FacialLandmarks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EJZwDvM21EV"
      },
      "source": [
        "# Facial Landmarks\n",
        "Experimentation with facial landmarks models and datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQFjoPKY-ZaY"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Circle\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras import layers\n",
        "from google.colab import drive\n",
        "import os\n",
        "from scipy.io import loadmat\n",
        "import random"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCIThEi--w0H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59327355-8cbb-4979-fbde-bb98c16ebac4"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXe9WUnF2_O3"
      },
      "source": [
        "Add your name and filepath to the project folder so that you can load the datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngxIBPhm1Q80"
      },
      "source": [
        "USER = \"John\"\n",
        "USER_FILEPATHS = {\n",
        "    \"John\" : \"/content/drive/MyDrive/CSC 566 Project\",\n",
        "    \"Jeremy\" : \"/content/drive/MyDrive/School/Undergrad/2021 Spring/CSC 566/CSC 566 Project\",\n",
        "    \"Ty\" : \"/content/drive/MyDrive/CSC 566 Project\"\n",
        "}\n",
        "PROJECT_FILEPATH = USER_FILEPATHS[USER]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcX_vDf3BcjR"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpSQj0yh2UTF"
      },
      "source": [
        "# Sanity check for loading data\n",
        "def display_landmarks(img, points):\n",
        "  \"\"\"\n",
        "  Displays portrait with landmark dots drawn on the face.\n",
        "  Assumes that points are in (68,2)\n",
        "  \"\"\"\n",
        "  #If we are reading from .mat files directly (2,68)\n",
        "  if points.shape[0] == 2:\n",
        "    points = points.transpose(1,0)\n",
        "  #If reading from model output\n",
        "  elif len(points.shape) == 1:\n",
        "    points = points.reshape(-1,2)\n",
        "  fig,ax = plt.subplots(1)\n",
        "  ax.set_aspect('equal')\n",
        "  ax.imshow(img)\n",
        "  for p in points:\n",
        "      circ = Circle(p)\n",
        "      ax.add_patch(circ)\n",
        "  plt.show()\n",
        "\n",
        "#points = loadmat(AFW_DATASET + \"/70037463_1.mat\")[\"pt2d\"]\n",
        "#img  = plt.imread( AFW_DATASET+\"/70037463_1.jpg\")\n",
        "\n",
        "#display_landmarks(img,points)\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-39rOKd3deP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97cde4aa-b6c1-498d-ac7c-9044389fe816"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "DATALOADER_BASE_PATH = PROJECT_FILEPATH + \"/datasets/tf-datasets\"\n",
        "USE_CROPPED_IMAGES = False\n",
        "DATASET_PREFIX = \"cropped-\" if USE_CROPPED_IMAGES else \"\"\n",
        "\n",
        "class DataLoader:\n",
        "\n",
        "  IMAGE_SHAPE = (256, 256, 3) if USE_CROPPED_IMAGES else (450, 450, 3)\n",
        "  LANDMARKS_SHAPE = (136,)\n",
        "\n",
        "  BATCH_SIZE = 16\n",
        "  TRAIN_PATHS = [f\"{DATALOADER_BASE_PATH}/{DATASET_PREFIX}train-{i}.tfrecord.gz\" for i in range(5)]\n",
        "  VAL_PATHS = [f\"{DATALOADER_BASE_PATH}/{DATASET_PREFIX}val-{i}.tfrecord.gz\" for i in range(5)]\n",
        "  TEST_PATHS = [f\"{DATALOADER_BASE_PATH}/{DATASET_PREFIX}test-{i}.tfrecord.gz\" for i in range(5)]\n",
        "\n",
        "  def load_datasets():\n",
        "    dl = DataLoader()\n",
        "    return (dl.__load_dataset(DataLoader.TRAIN_PATHS, DataLoader.BATCH_SIZE),\n",
        "            dl.__load_dataset(DataLoader.VAL_PATHS, DataLoader.BATCH_SIZE),\n",
        "            dl.__load_dataset(DataLoader.TEST_PATHS, 1))\n",
        "\n",
        "  def __load_dataset(self, filepath, batch_size):\n",
        "    dataset = tf.data.TFRecordDataset([filepath], compression_type=\"GZIP\")\n",
        "    dataset = dataset.map(self.__parse_example, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.map(self.__reshape_entry, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset.cache() # Cache the above map operations so they aren't re-run every epoch\n",
        "    return dataset\n",
        "\n",
        "\n",
        "  def __parse_example(self, record):\n",
        "    feature_names = {}\n",
        "    feature_names['image'] = tf.io.FixedLenSequenceFeature([],tf.float32, allow_missing=True)\n",
        "    feature_names['landmarks'] = tf.io.FixedLenSequenceFeature([],tf.float32, allow_missing=True)\n",
        "    return tf.io.parse_single_example(record, feature_names)\n",
        "\n",
        "  def __reshape_entry(self, entry):\n",
        "    image = tf.reshape(entry['image'], DataLoader.IMAGE_SHAPE)\n",
        "    landmarks = tf.reshape(entry['landmarks'], DataLoader.LANDMARKS_SHAPE)\n",
        "    return image, landmarks\n",
        "\n",
        "\n",
        "train_data, val_data, test_data = DataLoader.load_datasets()\n",
        "for record in train_data.take(27):\n",
        "  print(record[0].shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(450, 450, 3)\n",
            "(450, 450, 3)\n",
            "(450, 450, 3)\n",
            "(450, 450, 3)\n",
            "(450, 450, 3)\n",
            "(450, 450, 3)\n",
            "(450, 450, 3)\n",
            "(450, 450, 3)\n",
            "(450, 450, 3)\n",
            "(450, 450, 3)\n",
            "(450, 450, 3)\n",
            "(450, 450, 3)\n",
            "(450, 450, 3)\n",
            "(450, 450, 3)\n",
            "(450, 450, 3)\n",
            "(450, 450, 3)\n",
            "(450, 450, 3)\n",
            "(450, 450, 3)\n",
            "(450, 450, 3)\n",
            "(450, 450, 3)\n",
            "(450, 450, 3)\n",
            "(450, 450, 3)\n",
            "(450, 450, 3)\n",
            "(450, 450, 3)\n",
            "(450, 450, 3)\n",
            "(450, 450, 3)\n",
            "(450, 450, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwW6y33R9jmf"
      },
      "source": [
        "## Basic Benchmark Model\n",
        "From [this medium article](https://towardsdatascience.com/detecting-facial-features-using-deep-learning-2e23c8660a7a)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rX_z1wLO2YfP"
      },
      "source": [
        "def create_basic_landmark_model(input_shape, conv_range):\n",
        "  input_layer = layers.Input(input_shape)\n",
        "  x = input_layer\n",
        "  for exp in conv_range:\n",
        "    x = layers.Conv2D(2**exp, (3,3), 3, padding=\"same\")(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Conv2D(2**(exp - 1), (3,3), 3, padding=\"same\")(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPool2D(padding=\"same\")(x)\n",
        "    x = layers.Dropout(0.4)(x)\n",
        "  x = layers.Flatten()(x)\n",
        "  x = layers.Dense(500, activation=\"relu\")(x)\n",
        "  x = layers.Dense(90, activation=\"relu\")(x)\n",
        "  x = layers.Dense(68*2, activation=\"relu\")(x)\n",
        "  return Model(name=\"landmark_locator\", inputs=input_layer, outputs=x)\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfkeiXIDEfgL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "024a55e6-63d9-4c08-e9d5-dfc3d199f290"
      },
      "source": [
        "INPUT_SHAPE = (450,450,3)\n",
        "basic_model = create_basic_landmark_model(INPUT_SHAPE, range(5,8))\n",
        "basic_model.compile(optimizer=\"adam\", loss=\"mae\", metrics=[\"accuracy\"])\n",
        "basic_model.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"landmark_locator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 450, 450, 3)]     0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 150, 150, 32)      896       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_12 (LeakyReLU)   (None, 150, 150, 32)      0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 150, 150, 32)      128       \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 50, 50, 16)        4624      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_13 (LeakyReLU)   (None, 50, 50, 16)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 50, 50, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 25, 25, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 25, 25, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 9, 9, 64)          9280      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_14 (LeakyReLU)   (None, 9, 9, 64)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 9, 9, 64)          256       \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 3, 3, 32)          18464     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_15 (LeakyReLU)   (None, 3, 3, 32)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 3, 3, 32)          128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 2, 2, 32)          0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 2, 2, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 1, 1, 128)         36992     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_16 (LeakyReLU)   (None, 1, 1, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 1, 1, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 1, 1, 64)          73792     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_17 (LeakyReLU)   (None, 1, 1, 64)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 1, 1, 64)          256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 1, 1, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 1, 1, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 500)               32500     \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 90)                45090     \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 136)               12376     \n",
            "=================================================================\n",
            "Total params: 235,358\n",
            "Trainable params: 234,686\n",
            "Non-trainable params: 672\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ux1miy3D0oD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd063fed-190b-4f5d-d181-b516bdfbcbb4"
      },
      "source": [
        "for x in val_data.batch(64).take(1):\n",
        "  print(x[0].shape)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 450, 450, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQc5qiW1FzcD"
      },
      "source": [
        "SHOULD_TRAIN = False\n",
        "save_model_path = os.path.join(PROJECT_FILEPATH, \"models\", \"Base.h5\")\n",
        "if SHOULD_TRAIN:\n",
        "  cp_filepath = os.path.join(PROJECT_FILEPATH, \"models\", \"Base_Checkpoints\")\n",
        "  checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=cp_filepath, save_best_only=True)\n",
        "  basic_model.fit(train_data.batch(64), epochs=1000, verbose=1, callbacks=[checkpoint])\n",
        "  basic_model.save(save_model_path)\n",
        "else:\n",
        "  basic_model = tf.keras.models.load_model(save_model_path)\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nr31eAIQVGlu"
      },
      "source": [
        "### Analyze Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQJxDEObVSOx"
      },
      "source": [
        "images = test_data.map(lambda x,y: x)\n",
        "predictions = basic_model.predict(images.batch(64))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AShlJ00Vz81"
      },
      "source": [
        "#@title View Predicted Images\n",
        "image_index = 2 #@param {type:\"slider\", min:1, max:16, step:1}\n",
        "points = predictions * 450\n",
        "i = 0\n",
        "for x in images:\n",
        "  if (i == image_index):\n",
        "    display_landmarks(tf.reshape(x, DataLoader.IMAGE_SHAPE), points[image_index])\n",
        "    break\n",
        "  i += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odgsbWZDIwk8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}