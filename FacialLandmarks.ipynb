{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FacialLandmarks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Waidhoferj/CSC-566-Project/blob/heatmaps/FacialLandmarks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EJZwDvM21EV"
      },
      "source": [
        "# Facial Landmarks\n",
        "Experimentation with facial landmarks models and datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQFjoPKY-ZaY"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Circle\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras import layers\n",
        "from google.colab import drive\n",
        "import os\n",
        "from scipy.io import loadmat\n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCIThEi--w0H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83cea7bf-1dec-4fd2-d345-f0e34b14c15b"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXe9WUnF2_O3"
      },
      "source": [
        "Add your name and filepath to the project folder so that you can load the datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngxIBPhm1Q80"
      },
      "source": [
        "USER = \"Jeremy\"\n",
        "USER_FILEPATHS = {\n",
        "    \"John\" : \"/content/drive/MyDrive/CSC 566 Project\",\n",
        "    \"Jeremy\" : \"/content/drive/MyDrive/School/Undergrad/2021 Spring/CSC 566/CSC 566 Project\",\n",
        "    \"Ty\" : \"/content/drive/MyDrive/CSC 566 Project\"\n",
        "}\n",
        "PROJECT_FILEPATH = USER_FILEPATHS[USER]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcX_vDf3BcjR"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpSQj0yh2UTF"
      },
      "source": [
        "# Sanity check for loading data\n",
        "def display_landmarks(img, points):\n",
        "  \"\"\"\n",
        "  Displays portrait with landmark dots drawn on the face.\n",
        "  Assumes that points are in (68,2)\n",
        "  \"\"\"\n",
        "  #If we are reading from .mat files directly (2,68)\n",
        "  if points.shape[0] == 2:\n",
        "    points = points.transpose(1,0)\n",
        "  #If reading from model output\n",
        "  elif len(points.shape) == 1:\n",
        "    points = points.reshape(-1,2)\n",
        "  fig,ax = plt.subplots(1)\n",
        "  ax.set_aspect('equal')\n",
        "  ax.imshow(img)\n",
        "  for p in points:\n",
        "      circ = Circle(p)\n",
        "      ax.add_patch(circ)\n",
        "  plt.show()\n",
        "\n",
        "#points = loadmat(AFW_DATASET + \"/70037463_1.mat\")[\"pt2d\"]\n",
        "#img  = plt.imread( AFW_DATASET+\"/70037463_1.jpg\")\n",
        "\n",
        "#display_landmarks(img,points)\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-39rOKd3deP"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "USE_CROPPED_IMAGES = True\n",
        "USE_HEATMAPS = True\n",
        "\n",
        "DATALOADER_BASE_PATH = PROJECT_FILEPATH + \"/datasets/tf-datasets\"\n",
        "DATASET_PREFIX = \"cropped-\" if USE_CROPPED_IMAGES else \"\"\n",
        "\n",
        "class DataLoader:\n",
        "\n",
        "  IMAGE_SHAPE = (256, 256, 3) if USE_CROPPED_IMAGES else (450, 450, 3)\n",
        "  LANDMARKS_SHAPE = (136,)\n",
        "\n",
        "  HEATMAP_SHAPE = (128, 128)\n",
        "  HEATMAP_POINT_RADIUS = 0.15\n",
        "\n",
        "  BATCH_SIZE = 8\n",
        "  TRAIN_PATHS = [f\"{DATALOADER_BASE_PATH}/{DATASET_PREFIX}train-{i}.tfrecord.gz\" for i in range(5)]\n",
        "  VAL_PATHS = [f\"{DATALOADER_BASE_PATH}/{DATASET_PREFIX}val-{i}.tfrecord.gz\" for i in range(5)]\n",
        "  TEST_PATHS = [f\"{DATALOADER_BASE_PATH}/{DATASET_PREFIX}test-{i}.tfrecord.gz\" for i in range(5)]\n",
        "\n",
        "  def load_datasets():\n",
        "    dl = DataLoader()\n",
        "    return (dl.__load_dataset(DataLoader.TRAIN_PATHS, DataLoader.BATCH_SIZE),\n",
        "            dl.__load_dataset(DataLoader.VAL_PATHS, DataLoader.BATCH_SIZE),\n",
        "            dl.__load_dataset(DataLoader.TEST_PATHS, 1))\n",
        "\n",
        "  def __load_dataset(self, filepath, batch_size):\n",
        "    dataset = tf.data.TFRecordDataset([filepath], compression_type=\"GZIP\")\n",
        "    dataset = dataset.map(self.__parse_example, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.map(self.__reshape_entry, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    if USE_HEATMAPS:\n",
        "      imagesToHeatmapsDataset, heatmapsToPointsDataset = self.__convert_to_heatmaps(dataset, batch_size)\n",
        "      imagesToHeatmapsDataset.cache()\n",
        "      heatmapsToPointsDataset.cache()\n",
        "      return imagesToHeatmapsDataset, heatmapsToPointsDataset\n",
        "    else:\n",
        "      images = dataset.map(lambda x,y: x, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "      labels = dataset.map(lambda x,y: y, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "      images = images.batch(batch_size)\n",
        "      labels = labels.batch(batch_size)\n",
        "      dataset = tf.data.Dataset.zip((images, labels))\n",
        "      dataset.cache() # Cache the above map operations so they aren't re-run every epoch\n",
        "    return dataset\n",
        "\n",
        "\n",
        "  def __parse_example(self, record):\n",
        "    feature_names = {}\n",
        "    feature_names['image'] = tf.io.FixedLenSequenceFeature([],tf.float32, allow_missing=True)\n",
        "    feature_names['landmarks'] = tf.io.FixedLenSequenceFeature([],tf.float32, allow_missing=True)\n",
        "    return tf.io.parse_single_example(record, feature_names)\n",
        "\n",
        "  def __reshape_entry(self, entry):\n",
        "    image = tf.reshape(entry['image'], DataLoader.IMAGE_SHAPE)\n",
        "    landmarks = tf.reshape(entry['landmarks'], DataLoader.LANDMARKS_SHAPE)\n",
        "    return image, landmarks\n",
        "\n",
        "  def __convert_to_heatmaps(self, dataset, batch_size):\n",
        "    images = dataset.map(lambda x,y: x, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    labels = dataset.map(lambda x,y: y, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    heatmaps = labels.map(self.__convert_label_to_heatmap, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    heatmaps = heatmaps.batch(batch_size)\n",
        "\n",
        "    imagesToHeatmaps = tf.data.Dataset.zip((images.batch(batch_size), heatmaps))\n",
        "    heatmapsToPoints = tf.data.Dataset.zip((heatmaps, labels.batch(batch_size)))\n",
        "\n",
        "    return imagesToHeatmaps, heatmapsToPoints\n",
        "\n",
        "  @tf.autograph.experimental.do_not_convert\n",
        "  def __convert_label_to_heatmap(self, label):\n",
        "    heatmap = tf.zeros(DataLoader.HEATMAP_SHAPE)\n",
        "    coords = tf.range(DataLoader.HEATMAP_SHAPE[0]*DataLoader.HEATMAP_SHAPE[1], dtype=tf.float32)\n",
        "    coords = tf.reshape(coords, DataLoader.HEATMAP_SHAPE)\n",
        "    for i in range(0, 68):\n",
        "      heatmap += self.__add_point_to_heatmap(coords, label[2*i], label[2*i+1])\n",
        "    return heatmap\n",
        "\n",
        "  @tf.autograph.experimental.do_not_convert\n",
        "  def __add_point_to_heatmap(self, coords, x, y):\n",
        "    return tf.map_fn(fn=lambda i: self.__heatmap_point_dist(i, x, y, coords.shape[0], coords.shape[1]), elems=coords)\n",
        "\n",
        "  @tf.autograph.experimental.do_not_convert\n",
        "  def __heatmap_point_dist(self, i, x, y, w, h):\n",
        "    ix = (i%h)/w\n",
        "    iy = (i//h)/h\n",
        "    radius = DataLoader.HEATMAP_POINT_RADIUS\n",
        "    dist = tf.sqrt((x-ix)**2 + (y-iy)**2)\n",
        "    return tf.math.minimum(1.0, tf.math.maximum(0.0, 1.0 - (1/radius)*dist))\n",
        "\n",
        "\n",
        "train_data, val_data, test_data = DataLoader.load_datasets()\n",
        "\n",
        "#val_1st, val_2nd = val_data\n",
        "#\n",
        "#for i,record in enumerate(val_2nd):\n",
        "#  print(record[0].shape, record[1].shape)\n",
        "#  plt.gray()\n",
        "#  plt.imshow(record[0][0])\n",
        "#  break"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaaMQ8CSjsID"
      },
      "source": [
        "# Heatmap To Landmarks CNN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waY-H_8yhyxj",
        "outputId": "80c0e01b-633e-4b6f-e08b-76a3c768d0fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def create_heatmap_to_points_cnn(input_shape, output_shape):\n",
        "  input_layer = layers.Input(input_shape)\n",
        "  x = input_layer\n",
        "\n",
        "  x = layers.Reshape((input_shape[0], input_shape[1], 1))(x)\n",
        "\n",
        "  features = 16\n",
        "  for i in range(4):\n",
        "    x = layers.Conv2D(features, (3,3), 1, activation=\"relu\")(x)\n",
        "    x = layers.MaxPool2D(padding=\"same\")(x)\n",
        "    features *= 2\n",
        "\n",
        "  x = layers.Flatten()(x)\n",
        "  x = layers.Dense(512, activation=\"relu\")(x)\n",
        "  x = layers.Dense(output_shape[0], activation=\"relu\")(x)\n",
        "  return Model(name=\"heatmap_to_landmarks\", inputs=input_layer, outputs=x)\n",
        "\n",
        "\n",
        "heatmap_to_points_model = create_heatmap_to_points_cnn(DataLoader.HEATMAP_SHAPE, DataLoader.LANDMARKS_SHAPE)\n",
        "heatmap_to_points_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01), loss=\"mse\", metrics=[\"loss\"])\n",
        "heatmap_to_points_model.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"heatmap_to_landmarks\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_12 (InputLayer)        [(None, 128, 128)]        0         \n",
            "_________________________________________________________________\n",
            "reshape_11 (Reshape)         (None, 128, 128, 1)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_38 (Conv2D)           (None, 126, 126, 16)      160       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_33 (MaxPooling (None, 63, 63, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_39 (Conv2D)           (None, 61, 61, 32)        4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_34 (MaxPooling (None, 31, 31, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_40 (Conv2D)           (None, 29, 29, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_35 (MaxPooling (None, 15, 15, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_41 (Conv2D)           (None, 13, 13, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_36 (MaxPooling (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 512)               3211776   \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 136)               69768     \n",
            "=================================================================\n",
            "Total params: 3,378,696\n",
            "Trainable params: 3,378,696\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Qwo9-fgjrj1",
        "outputId": "8e35e957-dc85-4970-fd55-a1f3d36cf03b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        }
      },
      "source": [
        "_, train_ds = train_data\n",
        "_, val_ds = val_data\n",
        "\n",
        "#for i,record in enumerate(train_ds):\n",
        "#  print(record[0].shape, record[1].shape)\n",
        "#  plt.gray()\n",
        "#  plt.imshow(record[0][0])\n",
        "#  break\n",
        "\n",
        "heatmap_to_points_model.fit(train_ds.shuffle(8), epochs=30, validation_data=val_ds, verbose=1)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "204/204 [==============================] - 415s 2s/step - loss: 661.5781 - accuracy: 0.3145 - val_loss: 0.3796 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/30\n",
            "204/204 [==============================] - 410s 2s/step - loss: 0.6332 - accuracy: 0.4178 - val_loss: 0.3681 - val_accuracy: 0.5760\n",
            "Epoch 3/30\n",
            "204/204 [==============================] - 417s 2s/step - loss: 0.5689 - accuracy: 0.4437 - val_loss: 0.3635 - val_accuracy: 0.5240\n",
            "Epoch 4/30\n",
            "204/204 [==============================] - 414s 2s/step - loss: 0.5674 - accuracy: 0.4548 - val_loss: 0.3416 - val_accuracy: 0.5800\n",
            "Epoch 5/30\n",
            "204/204 [==============================] - 402s 2s/step - loss: 0.5441 - accuracy: 0.5680 - val_loss: 0.3367 - val_accuracy: 0.6240\n",
            "Epoch 6/30\n",
            "204/204 [==============================] - 399s 2s/step - loss: 0.5378 - accuracy: 0.5791 - val_loss: 0.3328 - val_accuracy: 0.6240\n",
            "Epoch 7/30\n",
            "204/204 [==============================] - 402s 2s/step - loss: 0.5373 - accuracy: 0.5655 - val_loss: 0.3311 - val_accuracy: 0.6000\n",
            "Epoch 8/30\n",
            "204/204 [==============================] - 404s 2s/step - loss: 0.5298 - accuracy: 0.5754 - val_loss: 0.3398 - val_accuracy: 0.6280\n",
            "Epoch 9/30\n",
            " 85/204 [===========>..................] - ETA: 3:21 - loss: 0.6305 - accuracy: 0.5618"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-4a53e37f4339>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#  break\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mheatmap_to_points_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_k66uwz08qJp",
        "outputId": "43791cac-6d2a-4fd0-84df-0dece923755a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "heatmap_to_points_model.evaluate(test_data[1])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "443/443 [==============================] - 104s 223ms/step - loss: 0.4050 - accuracy: 0.5734\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4050385057926178, 0.5733634233474731]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yZDpidv56-h"
      },
      "source": [
        "heatmap_to_points_model.save_weights(PROJECT_FILEPATH + \"/heatmap-to-points-model.h5\")"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwW6y33R9jmf"
      },
      "source": [
        "## Basic Benchmark Model\n",
        "From [this medium article](https://towardsdatascience.com/detecting-facial-features-using-deep-learning-2e23c8660a7a)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rX_z1wLO2YfP"
      },
      "source": [
        "def create_basic_landmark_model(input_shape, conv_range):\n",
        "  input_layer = layers.Input(input_shape)\n",
        "  x = input_layer\n",
        "  for exp in conv_range:\n",
        "    x = layers.Conv2D(2**exp, (3,3), 3, activation=\"relu\")(x)\n",
        "    x = layers.MaxPool2D(padding=\"same\")(x)\n",
        "  x = layers.Flatten()(x)\n",
        "  x = layers.Dense(500, activation=\"relu\")(x)\n",
        "  x = layers.Dense(90, activation=\"relu\")(x)\n",
        "  x = layers.Dense(68*2, activation=\"relu\")(x)\n",
        "  return Model(name=\"landmark_locator\", inputs=input_layer, outputs=x)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfkeiXIDEfgL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d25b2ce6-2b27-4fd2-b6e5-780d4e604911"
      },
      "source": [
        "INPUT_SHAPE = (450,450,3)\n",
        "basic_model = create_basic_landmark_model(INPUT_SHAPE, range(5,8))\n",
        "basic_model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"accuracy\"])\n",
        "basic_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"landmark_locator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 450, 450, 3)]     0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 150, 150, 32)      896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 75, 75, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 25, 25, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 4, 4, 128)         73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 2, 2, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 500)               256500    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 90)                45090     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 136)               12376     \n",
            "=================================================================\n",
            "Total params: 407,214\n",
            "Trainable params: 407,214\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQc5qiW1FzcD"
      },
      "source": [
        "basic_model.fit(train_data.shuffle(64), epochs=30, validation_data=val_data, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nr31eAIQVGlu"
      },
      "source": [
        "### Analyze Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukAD1nioU9WQ"
      },
      "source": [
        "pd.DataFrame(basic_model.history.history).plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQJxDEObVSOx"
      },
      "source": [
        "predictions = basic_model.predict(test_data.map(lambda x,y: x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AShlJ00Vz81"
      },
      "source": [
        "#@title View Predicted Images\n",
        "image_index = 3 #@param {type:\"slider\", min:1, max:16, step:1}\n",
        "points = predictions * 450\n",
        "i = 0\n",
        "for x in test_data.map(lambda x,y: x):\n",
        "  if (i == image_index):\n",
        "    display_landmarks(tf.reshape(x, DataLoader.IMAGE_SHAPE), points[image_index])\n",
        "    break\n",
        "  i += 1"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}