{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FacialLandmarks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Waidhoferj/CSC-566-Project/blob/PFLD/FacialLandmarks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EJZwDvM21EV"
      },
      "source": [
        "# Facial Landmarks\n",
        "Experimentation with facial landmarks models and datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQFjoPKY-ZaY"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Circle\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras import layers\n",
        "from google.colab import drive\n",
        "import os\n",
        "from scipy.io import loadmat\n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCIThEi--w0H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f4ee12c-fa1c-4ff5-b6a5-13cb5bd64722"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXe9WUnF2_O3"
      },
      "source": [
        "Add your name and filepath to the project folder so that you can load the datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngxIBPhm1Q80"
      },
      "source": [
        "USER = \"John\"\n",
        "USER_FILEPATHS = {\n",
        "    \"John\" : \"/content/drive/MyDrive/CSC 566 Project\",\n",
        "    \"Jeremy\" : \"/content/drive/MyDrive/School/Undergrad/2021 Spring/CSC 566/CSC 566 Project\",\n",
        "    \"Ty\" : \"/content/drive/MyDrive/CSC 566 Project\"\n",
        "}\n",
        "PROJECT_FILEPATH = USER_FILEPATHS[USER]\n",
        "DATASETS_PATH = os.path.join(PROJECT_FILEPATH, \"datasets\", \"300W-3D\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcX_vDf3BcjR"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpSQj0yh2UTF"
      },
      "source": [
        "# Sanity check for loading data\n",
        "def display_landmarks(img, points):\n",
        "  \"\"\"\n",
        "  Displays portrait with landmark dots drawn on the face.\n",
        "  Assumes that points are in (68,2)\n",
        "  \"\"\"\n",
        "  #If we are reading from .mat files directly (2,68)\n",
        "  if points.shape[0] == 2:\n",
        "    points = points.transpose(1,0)\n",
        "  #If reading from model output\n",
        "  elif len(points.shape) == 1:\n",
        "    points = points.reshape(-1,2)\n",
        "  fig,ax = plt.subplots(1)\n",
        "  ax.set_aspect('equal')\n",
        "  ax.imshow(img)\n",
        "  for p in points:\n",
        "      circ = Circle(p)\n",
        "      ax.add_patch(circ)\n",
        "  plt.show()\n",
        "\n",
        "#points = loadmat(AFW_DATASET + \"/70037463_1.mat\")[\"pt2d\"]\n",
        "#img  = plt.imread( AFW_DATASET+\"/70037463_1.jpg\")\n",
        "\n",
        "#display_landmarks(img,points)\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FViDYbR7IRA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffc9c80f-fbbd-4798-ee19-c4963d82ba70"
      },
      "source": [
        "def get_dataset_filenames(path):\n",
        "  return [os.path.join(path, name.split('.')[0]) for name in os.listdir(path) if name.endswith(\".mat\")]\n",
        "\n",
        "# Using only IBUG and AFW is fine because of their size. If you add HELEN\n",
        "# with the current setup, it will take like 10 min to load and 3 min per epoch\n",
        "datasets = [\n",
        "            \"IBUG\", \n",
        "            \"AFW\", \n",
        "            \"HELEN\", \n",
        "            # \"LFPW\"\n",
        "            ]\n",
        "filepaths = [get_dataset_filenames(f\"{PROJECT_FILEPATH}/datasets/300W-3D/{ds}\") for ds in datasets]\n",
        "filepaths = [item for sublist in filepaths for item in sublist] # Flattens the list\n",
        "\n",
        "def generate_pose():\n",
        "  for path in filepaths:\n",
        "    mat = loadmat(path + \".mat\")\n",
        "    yield mat[\"Pose_Para\"][0][0:3]\n",
        "\n",
        "def generate_image():\n",
        "  for path in filepaths:\n",
        "    img = plt.imread(path + \".jpg\")\n",
        "    yield img.astype(\"float32\") / 255.0\n",
        "\n",
        "def generate_landmarks():\n",
        "  for path in filepaths:\n",
        "    mat = loadmat(path + \".mat\")\n",
        "    yield mat[\"pt2d\"].transpose().reshape((-1, 136)).squeeze()\n",
        "\n",
        "\n",
        "def generate():\n",
        "  for i in range(0, len(filepaths), 16):\n",
        "    imgs = []\n",
        "    poses = []\n",
        "    landmarks = []\n",
        "    for path in filepaths[i:i+16]:\n",
        "      mat = loadmat(path + \".mat\")\n",
        "      img = plt.imread(path + \".jpg\")\n",
        "      img = img.astype(\"float32\") / 255.0\n",
        "      imgs.append(img)\n",
        "      pose =  mat[\"Pose_Para\"][0][0:3]\n",
        "      poses.append(pose)\n",
        "      points = mat[\"pt2d\"].transpose().reshape((-1, 136)).squeeze() / 450\n",
        "      landmarks.append(points)\n",
        "    yield ( np.array(imgs), {\"pose\" : np.array(poses), \"landmarks\" : np.array(landmarks)})\n",
        "\n",
        "\n",
        "ds = tf.data.Dataset.from_generator(\n",
        "    generate,\n",
        "    output_types=(tf.float64,{\"pose\": tf.float64, \"landmarks\": tf.float64})\n",
        ")\n",
        "\n",
        "TF_DATASETS = os.path.join(PROJECT_FILEPATH, \"datasets\", \"tf-datasets\")\n",
        "\n",
        "ds.cache()\n",
        "for record in ds.take(2):\n",
        "  print(record[0].numpy().shape)\n",
        "\n",
        "\n",
        "# for i in range(0, 5):\n",
        "#   record_batch_size = 500\n",
        "#   print(\"Test\", i*record_batch_size, min((i+1)*record_batch_size, filepaths.shape[0]), filepaths.shape[0])\n",
        "#   fp = filepaths[i*record_batch_size:min((i+1)*record_batch_size: filepaths.shape[0])]\n",
        "#   images, labels = get_examples(fp)\n",
        "#   print(images.shape, labels.shape)\n",
        "#   total_dataset_count = fp.shape[0]\n",
        "#   train_count = math.floor(total_dataset_count*0.65)\n",
        "#   val_count = math.floor(total_dataset_count*0.1)\n",
        "#   test_count = total_dataset_count - (train_count + val_count)\n",
        "#   convert_batch(f\"cropped-train-{i}.tfrecord.gz\", images, labels, 0, train_count)\n",
        "#   convert_batch(f\"cropped-val-{i}.tfrecord.gz\", images, labels, train_count+1, train_count+1+val_count)\n",
        "#   convert_batch(f\"cropped-test-{i}.tfrecord.gz\", images, labels, train_count+val_count+1, total_dataset_count)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(16, 450, 450, 3)\n",
            "(16, 450, 450, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8nXeY8TyBnk"
      },
      "source": [
        "## [PFLD Model](https://arxiv.org/pdf/1902.10859.pdf)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyI6el6RyMM_"
      },
      "source": [
        "\n",
        "def relu6(x):\n",
        "  return tf.clip_by_value(x, 0, 6)\n",
        "\n",
        "def mobilenet_block(x:layers.Layer, t=2, c=64, s=2):\n",
        "  \"\"\"\n",
        "  MoblieNetv2 residual block\n",
        "  Arguments\n",
        "  -----------\n",
        "    x: keras layer input\n",
        "    t: expansion factor based on size of input layer\n",
        "    c: number of input channels\n",
        "    s: stride width/height\n",
        "  \"\"\"\n",
        "  in_channels = x.shape[-1]\n",
        "  if in_channels != c:\n",
        "    x = layers.Conv2D(c, (1,1), strides=(1,1), padding=\"same\")(x) \n",
        "  l = layers.Conv2D(c * t, (1,1), strides=(1,1), padding=\"same\")(x) \n",
        "  l = layers.Activation(relu6)(l)\n",
        "  l = layers.BatchNormalization()(l)\n",
        "  l = layers.DepthwiseConv2D((s,s), padding=\"same\")(l)\n",
        "  l = layers.Activation(relu6)(l)\n",
        "  l = layers.BatchNormalization()(l)\n",
        "  l = layers.Conv2D(c, (1,1), strides=(1,1), padding=\"same\")(l)\n",
        "  return layers.Add()([l,x])\n",
        "\n",
        "def create_PFLD_model(input_shape, training=True):\n",
        "  \"\"\"\n",
        "  Builds a backbone model that takes in an image and outputs 68 landmark locations.\n",
        "  If training is true, it attaches the aux model\n",
        "  \"\"\"\n",
        "  input_layer = layers.Input(input_shape, name=\"model_input\")\n",
        "  x = layers.Conv2D(32,(3,3), strides=(2,2), activation=\"relu\", padding=\"same\")(input_layer)\n",
        "  x = layers.DepthwiseConv2D(32, strides=(1,1), activation=\"relu\")(x)\n",
        "  for _ in range(5):\n",
        "    x = mobilenet_block(x, t=2, c=32, s=2)\n",
        "  aux_in = mobilenet_block(x, t=2, c=64, s=2)\n",
        "  aux_out = create_aux_model(aux_in)\n",
        "  x = aux_in\n",
        "  for _ in range(6):\n",
        "    x = mobilenet_block(x, t=4, c=64, s=2)\n",
        "  x = mobilenet_block(x, t=2, c=16, s=1) \n",
        "\n",
        "  x = layers.Conv2D(32,(3,3), strides=(2,2), activation=\"relu\")(x)\n",
        "  x = layers.Conv2D(16,(7,7), strides=(4,4), activation=\"relu\")(x)\n",
        "  x = layers.Flatten()(x)\n",
        "  x = layers.Dense(64, activation=\"relu\", name=\"landmarks\")(x)\n",
        "  PFLD =  Model(name=\"PFLM_Backbone\", inputs=input_layer, outputs=[x, aux_out])\n",
        "  return PFLD\n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "def create_aux_model(input_layer: layers.Layer):\n",
        "  \"\"\"\n",
        "  Auxiliary model takes in the latent image representation from the MobileNet\n",
        "  layers and produces an estimate of the pitch, yaw and roll of the face\n",
        "  \"\"\"\n",
        "\n",
        "  x = input_layer\n",
        "  x = layers.Conv2D(128, (3,3), strides=(2,2), activation=\"relu\")(x)\n",
        "  x = layers.Conv2D(128, (3,3), strides=(1,1), activation=\"relu\")(x)\n",
        "  x = layers.Conv2D(32, (3,3), strides=(2,2), activation=\"relu\")(x)\n",
        "  x = layers.Conv2D(128, (7,7), strides=(1,1), activation=\"relu\")(x)\n",
        "  x = layers.Flatten()(x)\n",
        "  x = layers.Dense(32, activation=\"relu\")(x)\n",
        "  return layers.Dense(3, activation=\"relu\", name=\"pose\")(x)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGa1u3VmCSal"
      },
      "source": [
        "opt = tf.keras.optimizers.Adam()\n",
        "model = create_PFLD_model((450,450,3))\n",
        "\n",
        "model.compile(optimizer=opt, loss={\"landmarks\": \"mse\", \"pose\": \"mse\"}, metrics={\"landmarks\": tf.keras.metrics.RootMeanSquaredError(), \"pose\": tf.keras.metrics.RootMeanSquaredError()})\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT9ovTyQQjzb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "outputId": "46c213f2-c462-4ab4-dd86-458081eea8e2"
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "SHOULD_TRAIN = True\n",
        "if SHOULD_TRAIN:\n",
        "  cp_filepath = os.path.join(PROJECT_FILEPATH, \"models\", \"PFLD_Checkpoints\")\n",
        "  checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=cp_filepath, save_best_only=True)\n",
        "  model.fit(ds, epochs=50, batch_size=5, callbacks = [checkpoint])\n",
        "else:\n",
        "  model_path = os.path.join(PROJECT_FILEPATH, \"models\", \"PFLD.h5\")\n",
        "  model = load_model(model_path, custom_objects={\"relu6\": relu6})"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-f906e494aa9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mcp_filepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPROJECT_FILEPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"models\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"PFLD_Checkpoints\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcp_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPROJECT_FILEPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"models\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"PFLD.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[16,128,96,96] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node gradient_tape/PFLM_Backbone/conv2d_85/Conv2D/Conv2DBackpropInput (defined at <ipython-input-13-f906e494aa9d>:6) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_22079]\n\nFunction call stack:\ntrain_function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IZ5JacjGdyt"
      },
      "source": [
        "# pd.DataFrame(model.history.history).plot()\n",
        "images = ds.take(1).map(lambda x,y : x)\n",
        "landmarks, poses = model.predict(images)\n",
        "landmarks = landmarks *450"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MSTZoFbHLMw"
      },
      "source": [
        "#@title View Predicted Images\n",
        "image_index = 9 #@param {type:\"slider\", min:1, max:16, step:1}\n",
        "points = landmarks * 450\n",
        "i = 0\n",
        "for batch in images:\n",
        "  for img in batch:\n",
        "    if (i == image_index):\n",
        "      \n",
        "      display_landmarks(tf.reshape(img, (450,450,3)), landmarks[image_index])\n",
        "      break\n",
        "    i += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n46JksUYZiCn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}